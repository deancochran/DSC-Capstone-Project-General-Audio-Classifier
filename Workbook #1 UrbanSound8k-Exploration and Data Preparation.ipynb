{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.11"},"colab":{"name":"Workbook #1 UrbanSound8k-Exploration and Data Preparation.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ImtxQ1O0K3eA"},"source":["# **Welcome to my Data Science 500 Seminar Project**"]},{"cell_type":"markdown","metadata":{"id":"0AXPZwO5LBlS"},"source":["## Abstract:\n","\n","In this project our goal is to build a general audio classifier capable of recognising different sounds. The Sounds will be ambient noises from an urban environment like sirens traffic noises and construction. \n","\n","The purpose of this project is to understand the intricacies of sound, and what makes similar sounds different. Many people can readily recognise common sounds, for instance a dog bark, but people can identify these sounds only after theyâ€™ve learned the characteristics of each sound, after they have heard it through their entire life. \n","\n","I want to work with DSP (Digital Signal Processing) after college and this project serves as a way for me to be introduced into the field. Understanding how a singal is processed if fundamental to understanding how speech recognition, music, and audio prediction can be made.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bdn0X7_TsNGF"},"source":["## Workbook 1 - Extracting and Exploring UrbanSound8K Audio Data "]},{"cell_type":"markdown","metadata":{"id":"-QR5WmhkNH3R"},"source":["In this first workbook, we separate the over arching scope of this project Since we will be working with large sums of data we will need a way to process and store all the information we gather. \n","\n","This can take quite some time, and having a way to store our information is vital to increasing the efficency and overall production speed of our analysis. With this said, this workbooks purpose is to load in the data files we will be utilizing and store processed information from each data file into some kind of collection (folder inside a database) for us to call upon in the next steps of this project. \n","\n","This following workbook will take the information we store in our database and then create a predictive model to determine our capabilites of creating a predictive machine that can identify audio"]},{"cell_type":"markdown","metadata":{"id":"hQI6bOhdsNGI"},"source":["\n","\n","\n","\n","\n","First, here's the imports.\n","The audio processing is handled by a library called librosa\n","\n","Later examples use Keras framework and Tensorflow.\n","\n","We do need to import our standard numpy, matplotlib, as well as os which we can combine with mounting our drive to this colab session to reach our necessary files"]},{"cell_type":"code","metadata":{"id":"dUkfOSeysNGJ","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601485460791,"user_tz":240,"elapsed":1125,"user":{"displayName":"Dean Cochran","photoUrl":"","userId":"01347124029233554353"}},"outputId":"2ed4d3d3-840f-4625-bd4b-7df42d4c644b"},"source":["#BASIC IMPORTS\n","import glob\n","import os\n","import librosa\n","import librosa.display\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import specgram\n","%matplotlib inline\n","plt.style.use('ggplot')\n","\n","#MOUNTING OUR DRIVE\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sBqzZvCAfPLJ"},"source":["### This step is important so don't gloss over it\n","\n","Here, after we mount our drive to this session in colab we check to make sure we can find our directory where our project information lays."]},{"cell_type":"code","metadata":{"id":"tPD7Mwn2fkbJ","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1601485461431,"user_tz":240,"elapsed":1748,"user":{"displayName":"Dean Cochran","photoUrl":"","userId":"01347124029233554353"}},"outputId":"99cf3139-8a91-47b6-d575-5737d55fbdd9"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/ColabNotebooks/DSC 500 Seminar Project/My project/\")\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" data\t    UrbanSound8K\n"," pictures  'Workbook #1 UrbanSound8k-Exploration and Data Preparation.ipynb'\n"," samples   'Workbook #2 UrbanSound8k-FeedForwardNetwork Analysis.ipynb'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qdyv5vlF5YDp"},"source":["# NOTICE THE PATH NAME ABOVE\n","\n","Please make sure that the \"DSC 500 Seminar Project\" directory is inside a folder labeled \"ColabNotebooks\" in your realtive \"My Drive\" directory\n","\n","## The code cell above should output this:\n","'Copy of ProjectReport.gdoc'\n","\n","'Copy of ProjectReport.pdf'\n","\n"," pictures\n","\n"," samples\n","\n"," UrbanSound8K\n","\n","'UrbanSound8k-Exploration and Data Preparation NEEDS COMMENTS.ipynb'\n","\n","'UrbanSound8k-FeedForwardNetwork Analysis NEEDS COMMENTS.ipynb'\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zy5ZHjNX6ZEi"},"source":["## Trouble shooting:\n","*   Inside you home directory of google drive create a folder labeled \"ColabNotebooks\"\n","*   Inside ColabNotebooks, download or place the shared instance of \"DSC 500 Seminar Project\""]},{"cell_type":"code","metadata":{"id":"KT9MjW7tfuxw"},"source":["# lets save this path and alter it later in the project\n","projectDirectory = \"/content/drive/My Drive/ColabNotebooks/DSC 500 Seminar Project/My project\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRwrW9JIsNGR"},"source":["We'll begin by doing some basic visualisation of audio data from the UrbanSound8K dataset, a collection of 8732 short clips covering 10 different sounds from urban environments. \n","\n","\n","To do this we will need to crete a pretty extense amount of functions so that we can increase our productivity"]},{"cell_type":"code","metadata":{"id":"R4QrpOYRsNGT"},"source":["# function to load sounds in from there file locaiton\n","def load_sound_files(parent_dir, file_paths):\n","  #files added to an array\n","    raw_sounds = []\n","    for fp in file_paths:\n","        #with each file name we combine it with the parent diectory and add the designated file to the array\n","        X,sr = librosa.load(parent_dir + fp)\n","        raw_sounds.append(X)\n","    return raw_sounds\n","\n","# function to form a waveform plot for each file\n","def plot_waves(sound_names,raw_sounds):\n","    i = 1\n","    #creting plot instance\n","    fig = plt.figure(figsize=(25,10), dpi = 900)\n","    #every file has a title and a sound\n","    for n,f in zip(sound_names,raw_sounds):\n","        plt.subplot(2,5,i)\n","        #we use librose to create a waveform plot with each files data then label it with its title\n","        librosa.display.waveplot(np.array(f),sr=22050)\n","        plt.title(n.title())\n","        #increase the index\n","        i += 1\n","    #plot final waveform\n","    plt.suptitle('Figure 1: Waveplot',x=0.5, y=0.95,fontsize=18)\n","    plt.show()\n","    \n","\n","# function to form a spectrogram\n","#this finction is almost identical in style to that of the the waveform plotting function above\n","def plot_specgram(sound_names,raw_sounds):\n","    i = 1\n","    fig = plt.figure(figsize=(25,10), dpi = 900)\n","    for n,f in zip(sound_names,raw_sounds):\n","        plt.subplot(2,5,i)\n","        #notice here we create a spectrogram\n","        specgram(np.array(f), Fs=22050)\n","        plt.title(n.title())\n","        i += 1\n","    plt.suptitle('Figure 2: Spectrogram',x=0.5, y=0.95,fontsize=18)\n","    plt.show()\n","\n","#function to form a log_power_spectrogram (IDENTICAL TO FUNCTION ABOVE)\n","def plot_log_power_specgram(sound_names,raw_sounds):\n","    i = 1\n","    fig = plt.figure(figsize=(25,10), dpi = 900)\n","    for n,f in zip(sound_names,raw_sounds):\n","        plt.subplot(2,5,i)\n","        #logamplitude() changed to power_to_db in 2018\n","        D = librosa.power_to_db(np.abs(librosa.stft(f))**2, ref=np.max)\n","        librosa.display.specshow(D,x_axis='time' ,y_axis='log')\n","        plt.title(n.title())\n","        i += 1\n","    plt.suptitle('Figure 3: Log power spectrogram',x=0.5, y=0.95,fontsize=18)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMjVDOc2ljUm"},"source":["## Let's test our functions on some sample data that we have pre prepared"]},{"cell_type":"code","metadata":{"id":"az3UCybesNGg"},"source":["# names of the files in our us8k folder located in the \"samples directory\"\n","sound_file_paths = [\"aircon.wav\", \"carhorn.wav\", \"play.wav\", \"dogbark.wav\", \"drill.wav\",\n","                   \"engine.wav\",\"gunshots.wav\",\"jackhammer.wav\",\"siren.wav\",\"music.wav\"]\n","sound_names = [\"air conditioner\",\"car horn\",\"children playing\",\"dog bark\",\"drilling\",\"engine idling\",\n","               \"gun shot\",\"jackhammer\",\"siren\",\"street music\"]\n","#adding new path\n","parent_dir = projectDirectory + '/samples/us8k/'\n","\n","#loading sound data in to an array variable\n","raw_sounds = load_sound_files(parent_dir, sound_file_paths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vbw9FhkwsNGl"},"source":["Each sound can be visualised by how it changes over time. The classic view is the waveform, which shows the amplitude (relative loudness) of the sound at each successive time interval. \n","\n","Matplotlib provides an visualisation method called specgram - which calculates and plots the different intensities of the frequency spectrum. \n","\n","Another visualisation provided by Librosa is the log power spectrogram plotting. \n","\n","By looking at the plots shown in Figure 1, 2 and 3, we can see apparent differences between sound clips of different classes. These differences are what we want our deep learning system to learn and interpret."]},{"cell_type":"code","metadata":{"id":"ySY8qEHmsNGm","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1D9Kjv89Rwx8qxokapqIocdCxf6KCOBdS"},"executionInfo":{"status":"ok","timestamp":1601485676844,"user_tz":240,"elapsed":217057,"user":{"displayName":"Dean Cochran","photoUrl":"","userId":"01347124029233554353"}},"outputId":"3ad3880c-0dca-4959-dab8-158507f1dcef"},"source":["#using our pre-prepared functions to make visuals\n","plot_waves(sound_names, raw_sounds)\n","plot_specgram(sound_names, raw_sounds)\n","plot_log_power_specgram(sound_names, raw_sounds)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"mKwBIClZ8UIy","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pKxcohSUmV--jMQ0NJDYNq3JqoL_Tgs6"},"executionInfo":{"status":"ok","timestamp":1601485896038,"user_tz":240,"elapsed":102834,"user":{"displayName":"Dean Cochran","photoUrl":"","userId":"01347124029233554353"}},"outputId":"4f04a1d5-0a29-493e-8036-d88881cb35c6"},"source":["visual_title = [\"siren\"]\n","visual_file_paths = [\"siren.wav\"]\n","visual_sounds = load_sound_files(parent_dir, visual_file_paths)\n","\n","plot_waves(visual_title, visual_sounds)\n","plot_specgram(visual_title, visual_sounds)\n","plot_log_power_specgram(visual_title, visual_sounds)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"rIeM-EmnsNGs"},"source":["The images above show visualisations of the raw data, but some feature extraction is necessary. That means we'll always have the same features for each clip, regardless of how long or short it is. \n","\n","The librosa library comes with several methods , including:\n","\n","* Mel-frequency cepstral coefficients (MFCC) - https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n","* Chromagram of a short-time Fourier transform - projects bins representing the 12 distinct semitones (or chroma) of the musical octave http://labrosa.ee.columbia.edu/matlab/chroma-ansyn/\n","* Mel-scaled power spectrogram - uses https://en.wikipedia.org/wiki/Mel_scale to provide greater resolution for the more informative (lower) frequencies \n","* Octave-based spectral contrast (http://ieeexplore.ieee.org/document/1035731/)\n","* Tonnetz - estimates tonal centroids as coordinates in a six-dimensional interval space (https://sites.google.com/site/tonalintervalspace/)\n","\n","The results of the 5 extractions are then concatenated to give a consistent feature vector of 193 values for every audio clip we process.\n"]},{"cell_type":"code","metadata":{"id":"ypggbuPOsNGt"},"source":["#For each audio file we want to reduce the dimensionality\n","#to do this we need to implement all of the above feature extraction methods\n","def extract_feature(file_name):\n","    #give a single audio file we store the rate at which the audio was sampled at and store the length of the file\n","    X, sample_rate = librosa.load(file_name)\n","    print \"Features :\",len(X), \"sampled at \", sample_rate, \"hz\"\n","    #computing a Short-time Fourier transform\n","    stft = np.abs(librosa.stft(X))\n","    #computing the Mel-frequency cepstral coefficients\n","    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n","    #computing a Chromagram of the short-time Fourier transform\n","    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","    #computing a Mel-scaled power spectrogram\n","    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n","    #computing a Octave-based spectral contrast\n","    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n","    #computing the Tonnetz of the audio\n","    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n","    # return all features and values for files\n","    return mfccs,chroma,mel,contrast,tonnetz\n","\n","\n","# here for each audio file we use a loop to generate dimensional reductions on each audio file\n","def parse_audio_files(parent_dir,sub_dirs,file_ext='*.wav'):\n","  #creating an empty array of the length of are final feature values\n","    features, labels = np.empty((0,193)), np.empty(0)\n","    #starting our loop which reaches each audio file\n","    for label, sub_dir in enumerate(sub_dirs):\n","        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n","            #attempting to reduce all the audio files dimensions\n","            try:\n","                #extracting feature values\n","                mfccs, chroma, mel, contrast, tonnetz = extract_feature(fn)\n","                #Stacka arrays in sequence horizontally (column wise).\n","                ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n","                #Stacks arrays in sequence vertically (row wise)\n","                features = np.vstack([features,ext_features])\n","                #Append values to the end of an array.\n","                labels = np.append(labels, fn.split('fold')[1].split('-')[1])\n","            except:\n","                print(\"Error processing \" + fn + \" - skipping\")\n","    #return the array opf features and the array of labels for each feature(all integers)\n","    return np.array(features), np.array(labels, dtype = np.int)\n","\n","#one-hot encode all the labels\n","def one_hot_encode(labels):\n","    n_labels = len(labels)\n","    #finding length of all unique labels\n","    n_unique_labels = len(np.unique(labels))\n","    # Return a new array of given shape and type, filled with zeros.\n","    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n","    #Return evenly spaced values within a given interval (length of labels).\n","    one_hot_encode[np.arange(n_labels), labels] = 1\n","    return one_hot_encode\n","\n","\n","\n","#This is a complicated verification function i found on stack overflow\n","def assure_path_exists(path):\n","    mydir = os.path.join(os.getcwd(), path)\n","    #Returns True if path refers to an existing path or an open file descriptor.\n","    if not os.path.exists(mydir):\n","        os.makedirs(mydir)\n","        print('Initialized new directory')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fNsiffpsNGy"},"source":["### Here we see the dimensionality reduction in action, how a clip with 26168 data points is reduced into 193 features. \n","\n","We don't use a data file rather just one of our pre prepared samples instead"]},{"cell_type":"code","metadata":{"id":"xCloTkDssNGz"},"source":["#new path\n","sample_filename = projectDirectory+\"/samples/us8k/siren.wav\"\n","#extract features\n","mfccs, chroma, mel, contrast, tonnetz = extract_feature(sample_filename)\n","#Stacka arrays in sequence horizontally (column wise).\n","all_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n","#printing our how many features are extracted with each method\n","print \"MFCSS  = \", len(mfccs)\n","print \"Chroma = \", len(chroma)\n","print \"Mel = \", len(mel)\n","print \"Contrast = \", len(contrast)\n","print \"Tonnetz = \", len(tonnetz)\n","\n","# We want to see the dimensional reduction\n","data_points, _ = librosa.load(sample_filename)\n","#printting out the initial amount of data points the extraction was given\n","print \"IN: Initial Data Points =\", len(data_points), np.shape(data_points)\n","#printting out the end resukt of our extraction (the total # of feature values we will use)\n","print \"OUT: Total features =\", len(all_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0h_2jZRJsNG4"},"source":["### Another exploration we can perform is checking the balance of the dataset.\n","\n","This is useful to know, as we could inadvertently achieve good performance on just one class with many instances, and poor performance on all others."]},{"cell_type":"code","metadata":{"id":"cIXrK70usNG4"},"source":["# we want to see all the deifferent labels provided to us in our data so lets build a function\n","def get_labels(parent_dir,sub_dirs,file_ext='*.wav'):\n","  #creating empty array\n","    labels = np.empty(0)\n","    #for each audio path\n","    for label, sub_dir in enumerate(sub_dirs):\n","        #for each audio file\n","        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n","            #using cloab compatability to extract the class identification and label\n","            try:\n","                class_value = fn.split('fold')[1].split('-')[1]\n","                #appemnd info to array\n","                labels = np.append(labels, class_value)\n","            except:\n","                print(\"Error processing \" + fn + \" - skipping\")\n","    #return array\n","    return labels\n","\n","# put the path to the downloaded UrbanSound8K files here\n","raw_data_dir = projectDirectory+'/UrbanSound8K/audio/'\n","subsequent_fold = False\n","#our data is store in fold so that we can give it to our model in another workbook\n","#however we still need to parse through \"each fold\" so find our how much of each class is represented\n","#for each fold of data\n","for k in range(1,11):\n","    fold_name = 'fold' + str(k)\n","    #get labels array\n","    labels = get_labels(raw_data_dir, [fold_name])\n","    #Append labels already extracted to the collection of labels generated from the loop\n","    if subsequent_fold:\n","        all_labels = np.concatenate((all_labels, labels))\n","    else:\n","        all_labels = labels\n","        subsequent_fold = True\n","#now that we have all of our labels we need to separate them by their uniqueness    \n","unique, counts = np.unique(all_labels, return_counts=True)\n","\n","#plot the results\n","plt.figure(figsize=(18,4))\n","plt.bar(np.arange(len(unique)), counts, align='center')\n","plt.xticks(np.arange(len(unique)), sound_names)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"f2Q3wSnbsNHA"},"source":["## Still wondering why there is 2 workbooks??\n","\n","The code in the cell below can be run (once) to convert the raw audio files into much smaller numpy arrays. As this process is quite time consuming, we'd prefer to just do it once, and then load the numpy data when we want to do some training. "]},{"cell_type":"code","metadata":{"id":"3BNwel3WsNHA"},"source":["# use this to process the audio files into numpy arrays for easier use for our model in workbook #2\n","\n","def save_folds(data_dir):\n","    #for each existing fold of data stored in our directory we create 2 arrays (for x_test/train/val and ytest/train/val)\n","    for k in range(1,11):\n","        fold_name = 'fold' + str(k)\n","        print \"\\nSaving \" + fold_name\n","        #pull features and one hot encode our data\n","        features, labels = parse_audio_files(data_dir, [fold_name])\n","        labels = one_hot_encode(labels)\n","        # printing out each file so we know the function is working\n","        print \"Features of\", fold_name , \" = \", features.shape\n","        print \"Labels of\", fold_name , \" = \", labels.shape\n","\n","        #Here we save our files inside our data directory for easy storage\n","        feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n","        labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n","        np.save(feature_file, features)\n","        print \"Saved \" + feature_file\n","        np.save(labels_file, labels)\n","        print \"Saved \" + labels_file\n","  \n","# uncomment this to OVERWRITE and save the feature vectors\n","raw_data_dir = projectDirectory+\"/UrbanSound8K/audio/\"       \n","save_dir = projectDirectory+\"/data/us8k-np-ffn\"\n","#verifying this path exists\n","assure_path_exists(save_dir)\n","\n","#HERE WE OVERWRITE OUR EXISTING FILES\n","#saving our folds inside the data directory\n","save_folds(raw_data_dir)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD_jHeHRsNHF"},"source":["# End of Workbook 1"]},{"cell_type":"markdown","metadata":{"id":"aU8mWDpvsNHG"},"source":["--------------------------------------------------------------------------------------------------------------------------"]}]}